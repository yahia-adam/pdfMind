{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b0baeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain gradio chromadb pdfminer.six langchain-community notebook langchain_ollama langchain_chroma pymupdf openai langsmith langchain[openai] langchain-community openevals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9324b573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/Documents/adam/pdfMind/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 2. Evaluation\n",
    "from langsmith import Client, wrappers, traceable\n",
    "from openevals.llm import create_llm_as_judge\n",
    "from openevals.prompts import CORRECTNESS_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7912c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812175cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration\n",
    "DATA_PATH = \"data\"\n",
    "LLM_MODEL_NAME = \"mistral-nemo:latest\"\n",
    "EMBED_MODEL_NAME = \"nomic-embed-text\"\n",
    "CHROMA_PATH = f\"/home/adam/Documents/adam/pdfMind/chroma_db/{EMBED_MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affcf624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and vector store (this may take a moment)...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating embeddings and vector store (this may take a moment)...\")\n",
    "embeddings = OllamaEmbeddings(model=EMBED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e38aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Documents\n",
    "loader = DirectoryLoader(DATA_PATH, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8681c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fonction de nettoyage optimisée pour le Français\n",
    "import re\n",
    "def clean_french_text(text):\n",
    "    text = text.replace('\\xa0', ' ').replace('\\n', ' ')\n",
    "    text = text.replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
    "    text = re.sub(r'(\\w)-\\s+(\\w)', r'\\1\\2', text)\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "cleaned_documents = []\n",
    "for doc in documents:\n",
    "    doc.page_content = clean_french_text(doc.page_content)\n",
    "    if len(doc.page_content) > 20:\n",
    "        cleaned_documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0b6ca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 305 chunks.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"])\n",
    "chunks = text_splitter.split_documents(cleaned_documents)\n",
    "print(f\"Split into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "616a0ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Chroma vector store from documents\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=CHROMA_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01f1966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c5d159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=LLM_MODEL_NAME)\n",
    "\n",
    "@traceable()\n",
    "def rag_bot(question: str) -> dict:\n",
    "   docs = retriever.invoke(question)\n",
    "   docs_string = \"\".join(doc.page_content for doc in docs)\n",
    "   instructions = f\"\"\"Tu es un consultant expert en qualifications du bâtiment (Qualibat, RGE, Normes).\n",
    "Ta mission est d'expliquer les documents techniques fournis de manière pédagogique, structurée et synthétique.\n",
    "\n",
    "RÈGLES DE RÉDACTION (À SUIVRE IMPÉRATIVEMENT) :\n",
    "\n",
    "1. STRUCTURE VISUELLE :\n",
    "   - Commence toujours par une phrase d'introduction qui pose le contexte global.\n",
    "   - Utilise des **titres de sections** clairs pour séparer les thématiques.\n",
    "   - Utilise systématiquement des listes à puces (•) pour énumérer les détails.\n",
    "\n",
    "2. PÉDAGOGIE ET DÉTAILS :\n",
    "   - Si la question porte sur une **nomenclature ou un code** : Décortique la logique (ex: 1er chiffre = Famille). Donne un exemple concret (comme le code 2111 ou autre présent dans le contexte) pour illustrer.\n",
    "   - Si la question porte sur des **règles/normes** : Cite précisément les références (ex: NF X 46-010) et les durées de validité.\n",
    "   - Mets en **gras** les termes techniques importants, les chiffres clés et les concepts définis.\n",
    "\n",
    "3. TON :\n",
    "   - Professionnel, instructif et précis.\n",
    "   - Ne dis jamais \"D'après le contexte\", intègre l'information comme une connaissance établie.\n",
    "\n",
    "4. CONTRAINTE : Si la réponse n'est pas dans le contexte, dis \"Je ne sais pas\". N'invente rien.\n",
    "\n",
    "CONTEXTE DOCUMENTAIRE :\n",
    "\n",
    "{docs_string}\n",
    "\"\"\"\n",
    "\n",
    "   # langchain ChatModel will be automatically traced\n",
    "   ai_msg = llm.invoke([\n",
    "           {\"role\": \"system\", \"content\": instructions},\n",
    "           {\"role\": \"user\", \"content\": question},\n",
    "       ],\n",
    "   )\n",
    "   return {\"answer\": ai_msg.content, \"documents\": docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b26e7597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 9 familles d’activités sont :\n",
      "\n",
      "Famille 1 | PRÉPARATION DU SITE ET INFRASTRUCTURE Famille 2 | STRUCTURE ET GROS ŒUVRE Famille 3 | ENVELOPPE EXTÉRIEURE Famille 4 | CLOS - DIVISIONS - AMÉNAGEMENTS Famille 5 | ÉNERGIES ET FLUIDES Famille 6 | FINITIONS Famille 7 | ISOLATION THERMIQUE - ACOUSTIQUE - FRIGORIFIQUE Famille 8 | PERFORMANCE ÉNERGÉTIQUE Famille 9 | AGENCEMENT ET AMÉNAGEMENT\n"
     ]
    }
   ],
   "source": [
    "res = rag_bot(\"Quelles sont les 9 Famille d'activités ?\")\n",
    "print(res[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff4bea",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c1bb1d",
   "metadata": {},
   "source": [
    "### Exactitude (Correctness): Reponse vs reponse de reference\n",
    "- **Objectif** : Mesurer « la similarité/exactitude de la réponse de la chaîne RAG par rapport à une réponse de référence (vérité terrain) »\n",
    "- **Mode** : Nécessite une réponse de référence (vérité terrain) fournie dans un jeu de données\n",
    "- **Évaluateur** : Utiliser un LLM comme juge pour évaluer l'exactitude de la réponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a7f5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schéma de sortie pour la notation (Correctness)\n",
    "class CorrectnessGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Expliquez votre raisonnement pour le score obtenu\"]\n",
    "    correct: Annotated[bool, ..., \"Vrai si la réponse est correcte, Faux sinon.\"]\n",
    "\n",
    "# Instructions de notation (Prompt)\n",
    "correctness_instructions = \"\"\"Vous êtes un enseignant qui corrige un quiz. On vous donnera une QUESTION, la RÉPONSE DE RÉFÉRENCE (correcte) et la RÉPONSE de l'ÉLÈVE. Voici les critères de notation à suivre :\n",
    "\n",
    "(1) Notez les réponses de l'élève en vous basant UNIQUEMENT sur leur exactitude factuelle par rapport à la réponse de référence.\n",
    "\n",
    "(2) Assurez-vous que la réponse de l'élève ne contient aucune affirmation contradictoire.\n",
    "\n",
    "(3) Il est ACCEPTABLE que la réponse de l'élève contienne plus d'informations que la réponse de référence, tant qu'elle reste factuellement exacte par rapport à cette dernière.\n",
    "\n",
    "Exactitude :\n",
    "Une valeur d'exactitude \"True\" (Vrai) signifie que la réponse de l'élève répond à tous les critères.\n",
    "Une valeur d'exactitude \"False\" (Faux) signifie que la réponse de l'élève ne répond pas à tous les critères.\n",
    "\n",
    "Expliquez votre raisonnement étape par étape pour garantir que votre analyse et votre conclusion sont correctes. Évitez de donner simplement la réponse correcte dès le début.\"\"\"\n",
    "\n",
    "# Initialisation du LLM de notation\n",
    "grader_llm = ChatOllama(model=LLM_MODEL_NAME, temperature=0).with_structured_output(\n",
    "    CorrectnessGrade, method=\"json_schema\", strict=True\n",
    ")\n",
    "\n",
    "# Évaluateur\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Un évaluateur pour l'exactitude de la réponse RAG par rapport à une référence.\"\"\"\n",
    "    # Préparation du comparatif : Question vs Référence vs Réponse générée\n",
    "    answers = f\"\"\"\\\n",
    "QUESTION: {inputs['question']}\n",
    "RÉPONSE DE RÉFÉRENCE: {reference_outputs['answer']}\n",
    "RÉPONSE DE L'ÉLÈVE: {outputs['answer']}\"\"\"\n",
    "\n",
    "    # Exécution de l'évaluateur\n",
    "    grade = grader_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": correctness_instructions},\n",
    "        {\"role\": \"user\", \"content\": answers}\n",
    "    ])\n",
    "    return grade[\"correct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeba5e9",
   "metadata": {},
   "source": [
    "### Pertinence (Relevance): Réponse vs Entrée (Input)\n",
    "Le flux est similaire à celui décrit précédemment, mais nous examinons simplement les entrées (inputs) et les sorties (outputs) sans avoir besoin des réponses de référence (reference_outputs).\n",
    "\n",
    "Sans réponse de référence, nous ne pouvons pas évaluer l'exactitude factuelle, mais nous pouvons tout de même évaluer la pertinence — c'est-à-dire, déterminer si le modèle a répondu ou non à la question de l'utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0d5031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schéma de sortie pour la notation (Relevance)\n",
    "class RelevanceGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Expliquez votre raisonnement pour le score obtenu\"]\n",
    "    relevant: Annotated[\n",
    "        bool, ..., \"Indiquez si la réponse traite directement la question posée\"\n",
    "    ]\n",
    "\n",
    "# Instructions de notation (Prompt)\n",
    "relevance_instructions = \"\"\"Vous êtes un enseignant qui corrige un quiz. On vous donnera une QUESTION et une RÉPONSE de l'ÉLÈVE. Voici les critères de notation à suivre :\n",
    "\n",
    "(1) Assurez-vous que la RÉPONSE de l'ÉLÈVE est concise et pertinente par rapport à la QUESTION.\n",
    "(2) Assurez-vous que la RÉPONSE de l'ÉLÈVE aide réellement à répondre à la QUESTION.\n",
    "\n",
    "Pertinence :\n",
    "Une valeur de pertinence \"True\" (Vrai) signifie que la réponse de l'élève répond à tous les critères.\n",
    "Une valeur de pertinence \"False\" (Faux) signifie que la réponse de l'élève ne répond pas à tous les critères.\n",
    "\n",
    "Expliquez votre raisonnement étape par étape pour garantir que votre analyse et votre conclusion sont correctes. Évitez de donner simplement la réponse correcte dès le début.\"\"\"\n",
    "\n",
    "# Initialisation du LLM de notation\n",
    "relevance_llm = ChatOllama(model=LLM_MODEL_NAME, temperature=0).with_structured_output(\n",
    "    RelevanceGrade, method=\"json_schema\", strict=True\n",
    ")\n",
    "\n",
    "# Évaluateur\n",
    "def relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"Un évaluateur simple pour la pertinence et l'utilité de la réponse RAG.\"\"\"\n",
    "    # Préparation du contenu : Question vs Réponse générée\n",
    "    answer = f\"QUESTION: {inputs['question']}\\nRÉPONSE DE L'ÉLÈVE: {outputs['answer']}\"\n",
    "    \n",
    "    # Exécution de l'évaluateur\n",
    "    grade = relevance_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": relevance_instructions},\n",
    "        {\"role\": \"user\", \"content\": answer}\n",
    "    ])\n",
    "    return grade[\"relevant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bffd08",
   "metadata": {},
   "source": [
    "### Fidélité (Groundedness) : Réponse vs Documents Récupérés\n",
    "Une autre manière utile d'évaluer les réponses sans avoir besoin de réponses de référence consiste à vérifier si la réponse est justifiée par (ou « ancrée dans ») les documents récupérés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30262b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade output schema\n",
    "class GroundedGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    grounded: Annotated[\n",
    "        bool, ..., \"Provide the score on if the answer hallucinates from the documents\"\n",
    "    ]\n",
    "\n",
    "# Grade prompt\n",
    "grounded_instructions = \"\"\"Vous êtes un enseignant qui corrige un quiz. On vous donnera des FAITS et une RÉPONSE de l'ÉLÈVE. Voici les critères de notation à suivre :\n",
    "\n",
    "(1) Assurez-vous que la RÉPONSE de l'ÉLÈVE est ancrée (grounded) dans les FAITS fournis.\n",
    "(2) Assurez-vous que la RÉPONSE de l'ÉLÈVE ne contient pas d'informations \"hallucinées\" en dehors du cadre des FAITS.\n",
    "\n",
    "Fidélité :\n",
    "Une valeur \"True\" (Vrai) signifie que la réponse de l'élève répond à tous les critères.\n",
    "Une valeur \"False\" (Faux) signifie que la réponse de l'élève ne répond pas à tous les critères.\n",
    "\n",
    "Expliquez votre raisonnement étape par étape pour garantir que votre analyse et votre conclusion sont correctes. Évitez de donner simplement la réponse correcte dès le début.\"\"\"\n",
    "\n",
    "# Grader LLM\n",
    "grounded_llm = ChatOllama(model=LLM_MODEL_NAME, temperature=0).with_structured_output(\n",
    "    GroundedGrade, method=\"json_schema\", strict=True\n",
    ")\n",
    "\n",
    "# Evaluator\n",
    "def groundedness(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"A simple evaluator for RAG answer groundedness.\"\"\"\n",
    "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
    "    answer = f\"FACTS: {doc_string}\\nSTUDENT ANSWER: {outputs['answer']}\"\n",
    "    grade = grounded_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": grounded_instructions},\n",
    "        {\"role\": \"user\", \"content\": answer}\n",
    "    ])\n",
    "    return grade[\"grounded\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbb3e1",
   "metadata": {},
   "source": [
    "### Retrieval relevance: Retrieved docs vs input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a57556ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schéma de sortie pour la notation\n",
    "class RetrievalRelevanceGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Expliquez votre raisonnement pour le score obtenu\"]\n",
    "    relevant: Annotated[\n",
    "        bool, ..., \"Vrai si les documents récupérés sont pertinents par rapport à la question, Faux sinon\",\n",
    "    ]\n",
    "\n",
    "# Instructions de notation (Prompt)\n",
    "retrieval_relevance_instructions = \"\"\"Vous êtes un enseignant qui corrige un quiz. On vous donnera une QUESTION et un ensemble de FAITS fournis par l'élève. Voici les critères de notation à suivre :\n",
    "\n",
    "(1) Votre objectif est d'identifier les FAITS qui sont totalement sans rapport avec la QUESTION.\n",
    "(2) Si les faits contiennent n'importe quel mot-clé ou sens sémantique lié à la question, considérez-les comme pertinents.\n",
    "(3) Il est ACCEPTABLE que les faits contiennent CERTAINES informations sans rapport avec la question, tant que le critère (2) est respecté.\n",
    "\n",
    "Pertinence :\n",
    "Une valeur \"True\" (Vrai) signifie que les FAITS contiennent des mots-clés ou une signification sémantique liés à la QUESTION et sont donc pertinents.\n",
    "Une valeur \"False\" (Faux) signifie que les FAITS sont totalement sans rapport avec la QUESTION.\n",
    "\n",
    "Expliquez votre raisonnement étape par étape pour garantir que votre analyse et votre conclusion sont correctes. Évitez de donner simplement la réponse correcte dès le début.\"\"\"\n",
    "\n",
    "# Initialisation du LLM de notation\n",
    "retrieval_relevance_llm = ChatOllama(model=LLM_MODEL_NAME, temperature=0).with_structured_output(\n",
    "    RetrievalRelevanceGrade, method=\"json_schema\", strict=True\n",
    ")\n",
    "\n",
    "def retrieval_relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"Un évaluateur pour la pertinence des documents récupérés\"\"\"\n",
    "    # Extraction du contenu des documents récupérés\n",
    "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
    "    # Préparation du prompt pour le juge\n",
    "    answer = f\"FAITS: {doc_string}\\nQUESTION: {inputs['question']}\"\n",
    "    \n",
    "    # Exécution de l'évaluateur\n",
    "    grade = retrieval_relevance_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": retrieval_relevance_instructions},\n",
    "        {\"role\": \"user\", \"content\": answer}\n",
    "    ])\n",
    "    return grade[\"relevant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209610a",
   "metadata": {},
   "source": [
    "### Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ec416cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-70025c8d' at:\n",
      "https://smith.langchain.com/o/9dba41ba-edaf-4ddb-b5ed-eb28b05c0292/datasets/09f0b592-6a7e-488f-a980-a6b9d3507056/compare?selectedSessions=6c76d54b-c2f5-4c06-b067-916ed64158ac\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [38:54, 46.69s/it]\n"
     ]
    }
   ],
   "source": [
    "def target(inputs: dict) -> dict:\n",
    "    return rag_bot(inputs[\"question\"])\n",
    "\n",
    "eval_data = pd.read_csv('data/tests/samples.csv', sep=\";\")\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Mon Dataset d'Evaluation\"\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    client.create_dataset(dataset_name=dataset_name)\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in eval_data[\"question\"]],\n",
    "    outputs=[{\"answer\": a} for a in eval_data[\"answer\"]],\n",
    "    dataset_name=dataset_name\n",
    ")\n",
    "\n",
    "experiment_results = client.evaluate(\n",
    "    target,\n",
    "    data=dataset_name,\n",
    "    evaluators=[correctness, groundedness, relevance, retrieval_relevance],\n",
    "    experiment_prefix=\"test\",\n",
    "    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},\n",
    ")\n",
    "\n",
    "# experiment_results.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

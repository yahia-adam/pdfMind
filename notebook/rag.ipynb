{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0baeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain gradio chromadb pdfminer.six langchain-community notebook langchain_ollama langchain_chroma pymupdf openai langsmith langchain[openai] langchain-community openevals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9324b573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/Documents/adam/pdfMind/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pydantic_settings import BaseSettings\n",
    "from langchain_chroma import Chroma\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 2. Evaluation\n",
    "from langsmith import Client, wrappers, traceable\n",
    "from openevals.llm import create_llm_as_judge\n",
    "from openevals.prompts import CORRECTNESS_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7912c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "class Settings(BaseSettings):\n",
    "    # On récupère les valeurs brutes du .env d'abord\n",
    "    root_dir: str = os.getenv(\"ROOT_DIR\")\n",
    "    train_data_name: str = os.getenv(\"TRAIN_DATA_DIR\")\n",
    "    test_data_name: str = os.getenv(\"TEST_DATA_DIR\")\n",
    "    chroma_db_name: str = os.getenv(\"CHROMA_DB_DIR\")\n",
    "\n",
    "    # Models\n",
    "    rag_model_name: str = os.getenv(\"CHAT_MODEL_NAME\")\n",
    "    embedding_model_name: str = os.getenv(\"EMBEDDING_MODEL_NAME\")\n",
    "\n",
    "    # LangSmith\n",
    "    langsmith_endpoint: str = os.getenv(\"LANGSMITH_ENDPOINT\", \"https://api.smith.langchain.com\")\n",
    "    langsmith_api_key: str = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "    langsmith_tracing: bool = os.getenv(\"LANGSMITH_TRACING\", False)\n",
    "\n",
    "    # Utilisation de @property pour construire les chemins dynamiquement\n",
    "    @property\n",
    "    def train_data_dir(self) -> str:\n",
    "        return os.path.join(self.root_dir, self.train_data_name)\n",
    "\n",
    "    @property\n",
    "    def test_data_dir(self) -> str:\n",
    "        return os.path.join(self.root_dir, self.test_data_name)\n",
    "\n",
    "    @property\n",
    "    def chroma_db_dir(self) -> str:\n",
    "        return os.path.join(self.root_dir, self.chroma_db_name)\n",
    "\n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "affcf624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and vector store (this may take a moment)...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating embeddings and vector store (this may take a moment)...\")\n",
    "embeddings = OllamaEmbeddings(model=settings.embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e38aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Documents\n",
    "loader = DirectoryLoader(settings.train_data_dir, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8681c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fonction de nettoyage optimisée pour le Français\n",
    "import re\n",
    "def clean_french_text(text):\n",
    "    text = text.replace('\\xa0', ' ').replace('\\n', ' ')\n",
    "    text = text.replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
    "    text = re.sub(r'(\\w)-\\s+(\\w)', r'\\1\\2', text)\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "cleaned_documents = []\n",
    "for doc in documents:\n",
    "    doc.page_content = clean_french_text(doc.page_content)\n",
    "    if len(doc.page_content) > 20:\n",
    "        cleaned_documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b6ca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 305 chunks.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"])\n",
    "chunks = text_splitter.split_documents(cleaned_documents)\n",
    "print(f\"Split into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72a00f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des IDs\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.metadata['id'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "616a0ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Chroma vector store from documents\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=settings.chroma_db_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f1966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a722caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=settings.rag_model_name)\n",
    "\n",
    "@traceable()\n",
    "def rag_bot(question: str) -> dict:\n",
    "   docs = retriever.invoke(question)\n",
    "   \n",
    "   context_parts = []\n",
    "   for doc in docs:\n",
    "      chunk_id = doc.metadata.get('id', 'inconnu')\n",
    "      context_parts.append(f\"--- DÉBUT CHUNK ID: {chunk_id} ---\\n{doc.page_content}\\n--- FIN CHUNK ID: {chunk_id} ---\")\n",
    "   \n",
    "   docs_string = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "   instructions = f\"\"\"Tu es un consultant expert en qualifications du bâtiment (Qualibat, RGE, Normes).\n",
    "Ta mission est d'expliquer les documents techniques fournis de manière pédagogique, structurée et synthétique.\n",
    "\n",
    "RÈGLES DE RÉDACTION (À SUIVRE IMPÉRATIVEMENT) :\n",
    "\n",
    "1. CITATIONS OBLIGATOIRES (RÈGLE D'OR) :\n",
    "   - Pour chaque affirmation, fait technique ou chiffre, tu DOIS insérer l'ID du chunk correspondant entre crochets juste après l'information.\n",
    "   - Exemple : \"Le code 2111 correspond à la maçonnerie [45].\"\n",
    "   - Si une information provient de plusieurs chunks, liste les IDs : [12, 14].\n",
    "   - Ne crée jamais de citation si l'ID n'est pas explicitement dans le contexte fourni.\n",
    "\n",
    "2. STRUCTURE VISUELLE :\n",
    "   - Commence par une introduction globale.\n",
    "   - Utilise des **titres de sections** (##).\n",
    "   - Utilise des listes à puces (•).\n",
    "\n",
    "3. PÉDAGOGIE :\n",
    "   - Décortique la logique des codes (ex: 1er chiffre = Famille).\n",
    "   - Mets en **gras** les termes techniques et chiffres clés.\n",
    "\n",
    "4. TON ET CONTRAINTES :\n",
    "   - Professionnel et précis.\n",
    "   - Si la réponse n'est pas dans le contexte, dis \"Je ne sais pas\".\n",
    "\n",
    "CONTEXTE DOCUMENTAIRE :\n",
    "{docs_string}\n",
    "\"\"\"\n",
    "\n",
    "   ai_msg = llm.invoke([\n",
    "           {\"role\": \"system\", \"content\": instructions},\n",
    "           {\"role\": \"user\", \"content\": question},\n",
    "       ],\n",
    "   )\n",
    "   \n",
    "   return {\"status_code\": 200,  \"answer\": ai_msg.content, \"documents\": docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b26e7597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"La mention RGE (Reconnu Garant de l'Environnement) est un label attribué à des entreprises qualifiées pour réaliser des travaux d'économies d'énergie dans le bâtiment. Elle garantit que les entreprises bénéficient d'une qualification professionnelle et sont aptes à réaliser des travaux conformes aux réglementations en vigueur.\\n\\nElle permet également aux clients de bénéficier d'un travail de qualité et de faire des économies sur leurs factures énergétiques.\\n\\nLa mention RGE est délivrée pour certaines catégories de travaux, telles que l'installation de chaudières à haute performance énergétique ou les émetteurs électriques dont régulateurs de température [191].\\n\\nIl convient de noter que la mention RGE peut être attribuée automatiquement pour certaines catégories de travaux, comme les installations photovoltaïques [222]. Cependant, elle est souvent délivrée en accompagnement de qualifications attestant de la compétence des entreprises à réaliser des travaux dans les activités de la nomenclature concernées par ce dispositif : 21, 22, 23, 31, 32, 34, 35, 37, 38, 41, 43, 44, 45, 47, 51, 52, 53, 55, 61, 62, 63, 66, 71, 72, 91 [254].\\n\\nLa mention RGE est une garantie de qualité et de compétences pour les entreprises qui la possuem\",\n",
       " 'documents': [Document(id='94dcf73a-5420-436d-9654-1eed60d7534a', metadata={'id': 219, 'subject': '', 'creationdate': '2024-01-15T10:34:56+01:00', 'total_pages': 88, 'trapped': '', 'source': '/home/adam/Documents/adam/pdfMind/data/Nomenclature-Qualibat.pdf', 'creator': 'Adobe InDesign 18.5 (Windows)', 'page': 60, 'author': '', 'modDate': \"D:20240115104127+01'00'\", 'moddate': '2024-01-15T10:41:27+01:00', 'format': 'PDF 1.4', 'keywords': '', 'file_path': '/home/adam/Documents/adam/pdfMind/data/Nomenclature-Qualibat.pdf', 'creationDate': \"D:20240115103456+01'00'\", 'producer': 'Adobe PDF Library 17.0', 'title': ''}, page_content='. Mention \"RGE\" possible pour la catégorie de travaux : - Radiateurs électriques dont régulation *E.C. : exigence complémentaire'),\n",
       "  Document(id='06342168-a65f-4ccc-8185-267ee152d454', metadata={'moddate': '2024-01-15T10:41:27+01:00', 'creationdate': '2024-01-15T10:34:56+01:00', 'format': 'PDF 1.4', 'source': '/home/adam/Documents/adam/pdfMind/data/Nomenclature-Qualibat.pdf', 'id': 191, 'author': '', 'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 18.5 (Windows)', 'creationDate': \"D:20240115103456+01'00'\", 'trapped': '', 'keywords': '', 'page': 53, 'total_pages': 88, 'modDate': \"D:20240115104127+01'00'\", 'file_path': '/home/adam/Documents/adam/pdfMind/data/Nomenclature-Qualibat.pdf', 'subject': '', 'title': ''}, page_content='. Mention “RGE“ possible pour les catégories : - Catégorie “Chaudière à haute ou très haute performance énergétique ou à microcogénération gaz, dont régulateurs de températures“ - Catégorie “Émetteurs électriques dont régulateurs de température“'),\n",
       "  Document(id='87b06372-9244-4214-9a6d-1ece0e00f311', metadata={'moddate': '2024-01-15T10:41:27+01:00', 'creationDate': \"D:20240115103456+01'00'\", 'subject': '', 'modDate': \"D:20240115104127+01'00'\", 'id': 254, 'title': '', 'trapped': '', 'page': 74, 'creator': 'Adobe InDesign 18.5 (Windows)', 'author': '', 'format': 'PDF 1.4', 'producer': 'Adobe PDF Library 17.0', 'total_pages': 88, 'source': '/home/adam/Documents/adam/pdfMind/data/Nomenclature-Qualibat.pdf', 'creationdate': '2024-01-15T10:34:56+01:00', 'file_path': '/home/adam/Documents/adam/pdfMind/data/Nomenclature-Qualibat.pdf', 'keywords': ''}, page_content=\"1 2 3 4 5 6 7 8 9 75 FAMILLE 8 PERFORMANCE ÉNERGÉTIQUE 8 8 Activité 86 | EFFICACITÉ ÉNERGÉTIQUE Spécialité 860 MENTION “RGE“ E.P.* La mention “RGE“ est délivrée en accompagnement de certaines qualifications attestant de la compétence des entreprises à réaliser des travaux dans les activités de la nomenclature concernées par ce dispositif : 21, 22, 23, 31, 32, 34, 35, 37, 38, 41, 43, 44, 45, 47, 51, 52, 53, 55, 61, 62, 63, 66, 71, 72, 91. Elle traduit le savoir-faire d'une entreprise à mettre en œuvre et/ou installer des matériaux, composants et équipements qui concourent à l'amélioration de l'efficacité énergétique des bâtiments dans l'activité pour laquelle elle est qualifiée et sa connaissance des principes thermiques et énergétiques du bâtiment, lui permettant de préconiser les travaux correspondants\"),\n",
       "  Document(id='cf5abf5f-4d10-436d-a76d-538ea30f519a', metadata={'modDate': \"D:20240115104127+01'00'\", 'file_path': '/home/adam/Documents/adam/pdfMind/data/Nomenclature-Qualibat.pdf', 'format': 'PDF 1.4', 'moddate': '2024-01-15T10:41:27+01:00', 'subject': '', 'title': '', 'trapped': '', 'id': 222, 'creationdate': '2024-01-15T10:34:56+01:00', 'creationDate': \"D:20240115103456+01'00'\", 'author': '', 'creator': 'Adobe InDesign 18.5 (Windows)', 'page': 61, 'total_pages': 88, 'producer': 'Adobe PDF Library 17.0', 'source': '/home/adam/Documents/adam/pdfMind/data/Nomenclature-Qualibat.pdf', 'keywords': ''}, page_content=\". V Mention “RGE“ attribuée automatiquement : - Catégorie “Installations photovoltaïques“ Les prestations n'incluent pas l'éventuelle prestation d'intégration architecturale confiée à un architecte. *E.C. : exigence complémentaire\"),\n",
       "  Document(id='2c39526f-4c3c-4865-99f1-d32f38f83e71', metadata={'keywords': '', 'creationDate': \"D:20240115103456+01'00'\", 'file_path': '/home/adam/Documents/adam/pdfMind/data/Nomenclature-Qualibat.pdf', 'title': '', 'producer': 'Adobe PDF Library 17.0', 'page': 53, 'author': '', 'trapped': '', 'creator': 'Adobe InDesign 18.5 (Windows)', 'id': 190, 'subject': '', 'source': '/home/adam/Documents/adam/pdfMind/data/Nomenclature-Qualibat.pdf', 'moddate': '2024-01-15T10:41:27+01:00', 'format': 'PDF 1.4', 'modDate': \"D:20240115104127+01'00'\", 'total_pages': 88, 'creationdate': '2024-01-15T10:34:56+01:00'}, page_content=\". Mention “RGE“ attribuée automatiquement : - Catégorie “Appareils hydrauliques de chauffage ou de production d'eau chaude sanitaire fonctionnant au bois ou autres biomasses“ - Catégorie “Appareil indépendants de chauffage ou de production d'eau chaude sanitaire fonctionnant au bois ou autres biomasses “ «“ *E.C. : exigence complémentaire Qualification 5214 Installation de chauffage à gaz décentralisée : E.C.* aérotherme, radiants, générateur d'air chaud Réalisation d'installation de chauffage décentralisée (sans réseau de fluides caloporteurs) avec une production d'énergie fonctionnant au gaz (aérotherme, radiant, et générateur d'air chaud) pouvant comprendre : - Les travaux accessoires : platelage, supports d'appareils avec manutention et levage, petite métallerie, calorifugeage, raccordement électrique u matériel, isolation thermique et acoustique des ouvrages\")]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_bot(\"Qu'est-ce que la mention RGE ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff4bea",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c1bb1d",
   "metadata": {},
   "source": [
    "### Exactitude (Correctness): Reponse vs reponse de reference\n",
    "- **Objectif** : Mesurer « la similarité/exactitude de la réponse de la chaîne RAG par rapport à une réponse de référence (vérité terrain) »\n",
    "- **Mode** : Nécessite une réponse de référence (vérité terrain) fournie dans un jeu de données\n",
    "- **Évaluateur** : Utiliser un LLM comme juge pour évaluer l'exactitude de la réponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schéma de sortie pour la notation (Correctness)\n",
    "class CorrectnessGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Expliquez votre raisonnement pour le score obtenu\"]\n",
    "    correct: Annotated[bool, ..., \"Vrai si la réponse est correcte, Faux sinon.\"]\n",
    "\n",
    "# Instructions de notation (Prompt)\n",
    "correctness_instructions = \"\"\"Vous êtes un enseignant qui corrige un quiz. On vous donnera une QUESTION, la RÉPONSE DE RÉFÉRENCE (correcte) et la RÉPONSE de l'ÉLÈVE. Voici les critères de notation à suivre :\n",
    "\n",
    "(1) Notez les réponses de l'élève en vous basant UNIQUEMENT sur leur exactitude factuelle par rapport à la réponse de référence.\n",
    "\n",
    "(2) Assurez-vous que la réponse de l'élève ne contient aucune affirmation contradictoire.\n",
    "\n",
    "(3) Il est ACCEPTABLE que la réponse de l'élève contienne plus d'informations que la réponse de référence, tant qu'elle reste factuellement exacte par rapport à cette dernière.\n",
    "\n",
    "Exactitude :\n",
    "Une valeur d'exactitude \"True\" (Vrai) signifie que la réponse de l'élève répond à tous les critères.\n",
    "Une valeur d'exactitude \"False\" (Faux) signifie que la réponse de l'élève ne répond pas à tous les critères.\n",
    "\n",
    "Expliquez votre raisonnement étape par étape pour garantir que votre analyse et votre conclusion sont correctes. Évitez de donner simplement la réponse correcte dès le début.\"\"\"\n",
    "\n",
    "# Initialisation du LLM de notation\n",
    "grader_llm = ChatOllama(model=settings.rag_model_name, temperature=0).with_structured_output(\n",
    "    CorrectnessGrade, method=\"json_schema\", strict=True\n",
    ")\n",
    "\n",
    "# Évaluateur\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Un évaluateur pour l'exactitude de la réponse RAG par rapport à une référence.\"\"\"\n",
    "    # Préparation du comparatif : Question vs Référence vs Réponse générée\n",
    "    answers = f\"\"\"\\\n",
    "QUESTION: {inputs['question']}\n",
    "RÉPONSE DE RÉFÉRENCE: {reference_outputs['answer']}\n",
    "RÉPONSE DE L'ÉLÈVE: {outputs['answer']}\"\"\"\n",
    "\n",
    "    # Exécution de l'évaluateur\n",
    "    grade = grader_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": correctness_instructions},\n",
    "        {\"role\": \"user\", \"content\": answers}\n",
    "    ])\n",
    "    return grade[\"correct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeba5e9",
   "metadata": {},
   "source": [
    "### Pertinence (Relevance): Réponse vs Entrée (Input)\n",
    "Le flux est similaire à celui décrit précédemment, mais nous examinons simplement les entrées (inputs) et les sorties (outputs) sans avoir besoin des réponses de référence (reference_outputs).\n",
    "\n",
    "Sans réponse de référence, nous ne pouvons pas évaluer l'exactitude factuelle, mais nous pouvons tout de même évaluer la pertinence — c'est-à-dire, déterminer si le modèle a répondu ou non à la question de l'utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d5031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schéma de sortie pour la notation (Relevance)\n",
    "class RelevanceGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Expliquez votre raisonnement pour le score obtenu\"]\n",
    "    relevant: Annotated[\n",
    "        bool, ..., \"Indiquez si la réponse traite directement la question posée\"\n",
    "    ]\n",
    "\n",
    "# Instructions de notation (Prompt)\n",
    "relevance_instructions = \"\"\"Vous êtes un enseignant qui corrige un quiz. On vous donnera une QUESTION et une RÉPONSE de l'ÉLÈVE. Voici les critères de notation à suivre :\n",
    "\n",
    "(1) Assurez-vous que la RÉPONSE de l'ÉLÈVE est concise et pertinente par rapport à la QUESTION.\n",
    "(2) Assurez-vous que la RÉPONSE de l'ÉLÈVE aide réellement à répondre à la QUESTION.\n",
    "\n",
    "Pertinence :\n",
    "Une valeur de pertinence \"True\" (Vrai) signifie que la réponse de l'élève répond à tous les critères.\n",
    "Une valeur de pertinence \"False\" (Faux) signifie que la réponse de l'élève ne répond pas à tous les critères.\n",
    "\n",
    "Expliquez votre raisonnement étape par étape pour garantir que votre analyse et votre conclusion sont correctes. Évitez de donner simplement la réponse correcte dès le début.\"\"\"\n",
    "\n",
    "# Initialisation du LLM de notation\n",
    "relevance_llm = ChatOllama(model=settings.rag_model_name, temperature=0).with_structured_output(\n",
    "    RelevanceGrade, method=\"json_schema\", strict=True\n",
    ")\n",
    "\n",
    "# Évaluateur\n",
    "def relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"Un évaluateur simple pour la pertinence et l'utilité de la réponse RAG.\"\"\"\n",
    "    # Préparation du contenu : Question vs Réponse générée\n",
    "    answer = f\"QUESTION: {inputs['question']}\\nRÉPONSE DE L'ÉLÈVE: {outputs['answer']}\"\n",
    "    \n",
    "    # Exécution de l'évaluateur\n",
    "    grade = relevance_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": relevance_instructions},\n",
    "        {\"role\": \"user\", \"content\": answer}\n",
    "    ])\n",
    "    return grade[\"relevant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bffd08",
   "metadata": {},
   "source": [
    "### Fidélité (Groundedness) : Réponse vs Documents Récupérés\n",
    "Une autre manière utile d'évaluer les réponses sans avoir besoin de réponses de référence consiste à vérifier si la réponse est justifiée par (ou « ancrée dans ») les documents récupérés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30262b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade output schema\n",
    "class GroundedGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    grounded: Annotated[\n",
    "        bool, ..., \"Provide the score on if the answer hallucinates from the documents\"\n",
    "    ]\n",
    "\n",
    "# Grade prompt\n",
    "grounded_instructions = \"\"\"Vous êtes un enseignant qui corrige un quiz. On vous donnera des FAITS et une RÉPONSE de l'ÉLÈVE. Voici les critères de notation à suivre :\n",
    "\n",
    "(1) Assurez-vous que la RÉPONSE de l'ÉLÈVE est ancrée (grounded) dans les FAITS fournis.\n",
    "(2) Assurez-vous que la RÉPONSE de l'ÉLÈVE ne contient pas d'informations \"hallucinées\" en dehors du cadre des FAITS.\n",
    "\n",
    "Fidélité :\n",
    "Une valeur \"True\" (Vrai) signifie que la réponse de l'élève répond à tous les critères.\n",
    "Une valeur \"False\" (Faux) signifie que la réponse de l'élève ne répond pas à tous les critères.\n",
    "\n",
    "Expliquez votre raisonnement étape par étape pour garantir que votre analyse et votre conclusion sont correctes. Évitez de donner simplement la réponse correcte dès le début.\"\"\"\n",
    "\n",
    "# Grader LLM\n",
    "grounded_llm = ChatOllama(model=settings.rag_model_name, temperature=0).with_structured_output(\n",
    "    GroundedGrade, method=\"json_schema\", strict=True\n",
    ")\n",
    "\n",
    "# Evaluator\n",
    "def groundedness(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"A simple evaluator for RAG answer groundedness.\"\"\"\n",
    "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
    "    answer = f\"FACTS: {doc_string}\\nSTUDENT ANSWER: {outputs['answer']}\"\n",
    "    grade = grounded_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": grounded_instructions},\n",
    "        {\"role\": \"user\", \"content\": answer}\n",
    "    ])\n",
    "    return grade[\"grounded\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbb3e1",
   "metadata": {},
   "source": [
    "### Retrieval relevance: Retrieved docs vs input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57556ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schéma de sortie pour la notation\n",
    "class RetrievalRelevanceGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Expliquez votre raisonnement pour le score obtenu\"]\n",
    "    relevant: Annotated[\n",
    "        bool, ..., \"Vrai si les documents récupérés sont pertinents par rapport à la question, Faux sinon\",\n",
    "    ]\n",
    "\n",
    "# Instructions de notation (Prompt)\n",
    "retrieval_relevance_instructions = \"\"\"Vous êtes un enseignant qui corrige un quiz. On vous donnera une QUESTION et un ensemble de FAITS fournis par l'élève. Voici les critères de notation à suivre :\n",
    "\n",
    "(1) Votre objectif est d'identifier les FAITS qui sont totalement sans rapport avec la QUESTION.\n",
    "(2) Si les faits contiennent n'importe quel mot-clé ou sens sémantique lié à la question, considérez-les comme pertinents.\n",
    "(3) Il est ACCEPTABLE que les faits contiennent CERTAINES informations sans rapport avec la question, tant que le critère (2) est respecté.\n",
    "\n",
    "Pertinence :\n",
    "Une valeur \"True\" (Vrai) signifie que les FAITS contiennent des mots-clés ou une signification sémantique liés à la QUESTION et sont donc pertinents.\n",
    "Une valeur \"False\" (Faux) signifie que les FAITS sont totalement sans rapport avec la QUESTION.\n",
    "\n",
    "Expliquez votre raisonnement étape par étape pour garantir que votre analyse et votre conclusion sont correctes. Évitez de donner simplement la réponse correcte dès le début.\"\"\"\n",
    "\n",
    "# Initialisation du LLM de notation\n",
    "retrieval_relevance_llm = ChatOllama(model=settings.rag_model_name, temperature=0).with_structured_output(\n",
    "    RetrievalRelevanceGrade, method=\"json_schema\", strict=True\n",
    ")\n",
    "\n",
    "def retrieval_relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"Un évaluateur pour la pertinence des documents récupérés\"\"\"\n",
    "    # Extraction du contenu des documents récupérés\n",
    "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
    "    # Préparation du prompt pour le juge\n",
    "    answer = f\"FAITS: {doc_string}\\nQUESTION: {inputs['question']}\"\n",
    "    \n",
    "    # Exécution de l'évaluateur\n",
    "    grade = retrieval_relevance_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": retrieval_relevance_instructions},\n",
    "        {\"role\": \"user\", \"content\": answer}\n",
    "    ])\n",
    "    return grade[\"relevant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209610a",
   "metadata": {},
   "source": [
    "### Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec416cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(inputs: dict) -> dict:\n",
    "    return rag_bot(inputs[\"question\"])\n",
    "\n",
    "eval_data = pd.read_csv('data/tests/samples.csv', sep=\";\")\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Dataset d'Evaluation\"\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    client.create_dataset(dataset_name=dataset_name)\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in eval_data[\"question\"]],\n",
    "    outputs=[{\"answer\": a} for a in eval_data[\"answer\"]],\n",
    "    dataset_name=dataset_name\n",
    ")\n",
    "\n",
    "experiment_results = client.evaluate(\n",
    "    target,\n",
    "    data=dataset_name,\n",
    "    evaluators=[correctness, groundedness, relevance, retrieval_relevance],\n",
    "    experiment_prefix=\"test\",\n",
    "    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},\n",
    ")\n",
    "\n",
    "# experiment_results.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
